{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 6 - Math for AI, AI23 @ HCMUS\n",
    "- 23122013 - Đinh Đức Tài\n",
    "- 23122002 - Nguyễn Đình Hà Dương\n",
    "- 23122004 - Nguyễn Lê Hoàng Trung\n",
    "- 23122014 - Hoàng Minh Trung\n",
    "\n",
    "## [Lab1] Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Import libs, define DataProcessor and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1: Import libs: Numpy, Pandas, Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T14:30:19.128634Z",
     "start_time": "2025-03-18T14:30:19.124752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries: numpy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2: Create class DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "        self.original = True\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from the CSV file.\"\"\"\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        print(\"Data loaded successfully!\")\n",
    "        return self.data\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print a summary of the data.\"\"\"\n",
    "        print(\"Number of rows:\", len(self.data))\n",
    "        print(\"Column names:\", self.data.columns.tolist())\n",
    "        return self.data.describe()\n",
    "    \n",
    "    def head(self, n = 5):\n",
    "        \"\"\"Return the first n rows of the data.\"\"\"\n",
    "        return self.data.head(n)\n",
    "    \n",
    "    def null_info(self):\n",
    "        \"\"\"Print information about missing values.\"\"\"\n",
    "        print(\"\\nNumber of rows with NaN values:\", self.data.isna().any(axis=1).sum())\n",
    "\n",
    "    def get_column_initial_info(self):\n",
    "        print(\"\\nInformation about the columns:\")\n",
    "        column_info = pd.DataFrame({\n",
    "            'Column Name': self.data.columns,\n",
    "            'Description': [\n",
    "                \"Hãng xe\", \"Mẫu xe\", \"Giá xe (VNĐ)\", \"Năm sản xuất\", \"Số km đã đi\", \n",
    "                \"Loại nhiên liệu\", \"Hộp số\", \"Địa điểm bán\", \"Màu xe\", \"Số chủ sở hữu trước đó\", \n",
    "                \"Loại người bán\", \"Dung tích động cơ (cc)\", \"Công suất tối đa (bhp)\", \n",
    "                \"Mô-men xoắn tối đa (Nm)\", \"Hệ dẫn động\", \"Chiều dài xe (mm)\", \n",
    "                \"Chiều rộng xe (mm)\", \"Chiều cao xe (mm)\", \"Số chỗ ngồi\", \n",
    "                \"Dung tích bình nhiên liệu (lít)\"\n",
    "            ],\n",
    "            'Data Type': self.data.dtypes.values,\n",
    "            'Number of NaN': self.data.isna().sum().values,\n",
    "            'Unique Values': self.data.nunique().values,\n",
    "            'Most Frequent Value': self.data.mode().iloc[0].values,\n",
    "        })\n",
    "\n",
    "        return column_info\n",
    "    \n",
    "    def get_column_after_transform_info(self):\n",
    "        print(\"\\nInformation about the columns:\")\n",
    "        column_info = pd.DataFrame({\n",
    "            'Column Name': self.data.columns,\n",
    "            'Description': [\n",
    "                \"Hãng xe\", \"Mẫu xe\", \"Giá xe (VNĐ)\", \"Năm sản xuất\", \"Số km đã đi\", \n",
    "                \"Loại nhiên liệu\", \"Hộp số\", \"Địa điểm bán\", \"Màu xe\", \"Số chủ sở hữu trước đó\", \n",
    "                \"Loại người bán\", \"Dung tích động cơ (cc)\", \"Công suất tối đa (bhp)\", \n",
    "                \"Mô-men xoắn tối đa (Nm)\", \"Hệ dẫn động\", \"Chiều dài xe (mm)\", \n",
    "                \"Chiều rộng xe (mm)\", \"Chiều cao xe (mm)\", \"Số chỗ ngồi\", \n",
    "                \"Dung tích bình nhiên liệu (lít)\", 'Vòng tua tại Công suất tối đa (rpm)',\n",
    "                'Vòng tua tại Mô-men xoắn tối đa (rpm)',\n",
    "            ],\n",
    "            'Data Type': self.data.dtypes.values,\n",
    "            'Number of NaN': self.data.isna().sum().values,\n",
    "            'Unique Values': self.data.nunique().values,\n",
    "            'Most Frequent Value': self.data.mode().iloc[0].values,\n",
    "        })\n",
    "\n",
    "        return column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3: Get infomation about unique values functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_unique_values(self):\n",
    "    print(\"\\nUnique values of some columns:\")\n",
    "    print(\"Fuel Type:\", self.data['Fuel Type'].unique())\n",
    "    print(\"Transmission:\", self.data['Transmission'].unique())\n",
    "    print(\"Seller Type:\", self.data['Seller Type'].unique())\n",
    "    print(\"Drivetrain:\", self.data['Drivetrain'].unique())\n",
    "\n",
    "    print(\"Owner:\", self.data['Owner'].unique())\n",
    "    print(\"Seating Capacity:\", self.data['Seating Capacity'].unique())\n",
    "\n",
    "def unique_values(self):\n",
    "    object_columns = self.data.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # List of object columns and their unique values\n",
    "    object_columns_list = [(col, self.data[col].nunique()) for col in object_columns]\n",
    "\n",
    "    # List of numeric columns and their unique values\n",
    "    numeric_columns_list = [(col, self.data[col].nunique()) for col in numeric_columns]\n",
    "\n",
    "    print(\"Object Columns and number of unique values: {}\".format(len(object_columns_list)))\n",
    "    print(object_columns_list)\n",
    "\n",
    "    print(\"\\nNumeric Columns and number of unique values: {}\".format(len(numeric_columns_list)))\n",
    "    print(numeric_columns_list)\n",
    "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
    "    return numeric_columns, object_columns\n",
    "\n",
    "DataProcessor.get_some_unique_values = get_some_unique_values\n",
    "DataProcessor.unique_values = unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4: Clean and transform data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(self):\n",
    "    \"\"\"Clean the data by handling missing values and duplicates.\"\"\"\n",
    "    # Handle missing values\n",
    "    # Fill numeric columns with their mean\n",
    "    numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    self.data[numeric_columns] = self.data[numeric_columns].fillna(self.data[numeric_columns].mean().astype(int))\n",
    "\n",
    "    # Fill categorical columns with the most frequent value\n",
    "    categorical_columns = self.data.select_dtypes(include=['object']).columns\n",
    "    self.data[categorical_columns] = self.data[categorical_columns].fillna(self.data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "    # Remove duplicates\n",
    "    self.data = self.data.drop_duplicates()\n",
    "\n",
    "    # Reset index after cleaning\n",
    "    self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print summary after cleaning\n",
    "    print(\"Data cleaned successfully!\")\n",
    "    print(\"Number of rows after cleaning:\", len(self.data))\n",
    "    print(\"Number of missing values after cleaning:\", self.data.isna().sum().sum())\n",
    "\n",
    "def transform_data(self):\n",
    "    \"\"\"Transform data by standardizing specific columns.\"\"\"\n",
    "    # 'Engine' ('cc') -> float\n",
    "    self.data['Engine'] = self.data['Engine'].astype(str).str.replace(' cc', '').astype(float)\n",
    "\n",
    "    # Extract RPM values from the original string values before conversion\n",
    "    if (self.original is True):\n",
    "        self.data['rpm at Max Power'] = (\n",
    "            self.data['Max Power']\n",
    "            .astype(str)\n",
    "            .str.extract(r'@\\s*(\\d+)\\s*rpm', expand=False)\n",
    "        )\n",
    "        self.data['rpm at Max Torque'] = (\n",
    "            self.data['Max Torque']\n",
    "            .astype(str)\n",
    "            .str.extract(r'@\\s*(\\d+)\\s*rpm', expand=False)\n",
    "        )\n",
    "    self.original = False\n",
    "\n",
    "    # Fill missing values with the most frequent value\n",
    "    self.data['rpm at Max Power'] = self.data['rpm at Max Power'].fillna(self.data['rpm at Max Power'].mode().iloc[0])\n",
    "    self.data['rpm at Max Torque'] = self.data['rpm at Max Torque'].fillna(self.data['rpm at Max Torque'].mode().iloc[0])\n",
    "\n",
    "    # 'rpm at Max Power' -> int\n",
    "    self.data['rpm at Max Power'] = self.data['rpm at Max Power'].astype(int)\n",
    "\n",
    "    # 'rpm at Max Torque' -> int\n",
    "    self.data['rpm at Max Torque'] = self.data['rpm at Max Torque'].astype(int)\n",
    "    \n",
    "    # 'Max Power' ('bhp') -> int\n",
    "    self.data['Max Power'] = self.data['Max Power'].astype(str).str.extract(r'(\\d+)', expand=False).astype(int)\n",
    "\n",
    "    # 'Max Torque' ('Nm') -> int\n",
    "    self.data['Max Torque'] = self.data['Max Torque'].astype(str).str.extract(r'(\\d+)', expand=False).astype(int)\n",
    "\n",
    "    # 'Seating Capacity' -> int\n",
    "    self.data['Seating Capacity'] = self.data['Seating Capacity'].astype(int)\n",
    "\n",
    "    # 'Fuel Tank Capacity' -> int\n",
    "    self.data['Fuel Tank Capacity'] = self.data['Fuel Tank Capacity'].astype(int)\n",
    "\n",
    "    # 'Owner' -> int\n",
    "    self.data['Owner'] = self.data['Owner'].map({\n",
    "        'UnRegistered Car': 0,\n",
    "        'First': 1,\n",
    "        'Second': 2,\n",
    "        'Third': 3,\n",
    "        'Fourth': 4,\n",
    "        '4 or More': 5,\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 3,\n",
    "        4: 4,\n",
    "        5: 5,\n",
    "    })\n",
    "\n",
    "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(\"Data transformed successfully!\")\n",
    "    print(\"\\nData Transformation Details:\")\n",
    "    print(\"- 'Engine' (cc) converted to float.\")\n",
    "    print(\"- 'Max Power' (bhp) converted to integer.\")\n",
    "    print(\"- 'Max Torque' (Nm) converted to integer.\")\n",
    "    print(\"- Add 'rpm at Max Power' and converted to integer.\")\n",
    "    print(\"- Add 'rpm at Max Torque' and converted to integer.\")\n",
    "    print(\"- 'Seating Capacity' converted to integer.\")\n",
    "    print(\"- 'Fuel Tank Capacity' converted to integer.\")\n",
    "    print(\"- 'Owner' converted to numerical categories.\")\n",
    "\n",
    "DataProcessor.clean_data = clean_data\n",
    "DataProcessor.transform_data = transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5: Data visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(self, width=12, height=8):\n",
    "    # Compute the correlation matrix using only numeric features\n",
    "    corr_matrix = self.data[self.numeric_columns].corr()\n",
    "\n",
    "    # Plot the correlation matrix using matplotlib\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr_matrix)), corr_matrix.columns)\n",
    "    plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "\n",
    "    # Annotate the matrix with correlation coefficients\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            plt.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_of_numeric_columns(self):\n",
    "    # Plot histograms for all numeric columns\n",
    "    num_cols = self.numeric_columns\n",
    "\n",
    "    num_cols_count = len(num_cols)\n",
    "    n_cols = 3  # Number of columns in the figure\n",
    "    n_rows = (num_cols_count + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3.5 * n_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        axes[i].hist(self.data[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Histogram of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def box_plot_for_object_columns(self):\n",
    "    for col in self.object_columns:\n",
    "        if (col == 'Model'): \n",
    "            continue\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        categories = self.data[col].unique()\n",
    "        groups = [self.data.loc[self.data[col] == category, 'Price'] for category in categories]\n",
    "        plt.boxplot(groups, patch_artist=True, tick_labels=categories)\n",
    "        plt.title(f'Box Plot: Price by {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Price')\n",
    "        if (col == 'Location'):\n",
    "            plt.xticks(rotation=90)\n",
    "        elif (col == 'Fuel Type' or col == 'Transmission' or col == 'Seller Type' or col == 'Drivetrain'):\n",
    "            plt.xticks(rotation=0)\n",
    "        else:\n",
    "            plt.xticks(rotation=60)\n",
    "        plt.show()\n",
    "\n",
    "def scatter_plot_for_numeric_columns(self):\n",
    "    cols = [col for col in self.numeric_columns if col != 'Price']\n",
    "    n_plots = len(cols)\n",
    "    n_cols = 3\n",
    "    n_rows = int(n_plots / n_cols) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        axes[i].scatter(self.data[col], self.data['Price'], alpha=0.5, color='blue', edgecolors='k')\n",
    "        axes[i].set_title(f'Relationship between Price and {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Price')\n",
    "        axes[i].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "DataProcessor.plot_corr_matrix = plot_corr_matrix\n",
    "DataProcessor.plot_distribution_of_numeric_columns = plot_distribution_of_numeric_columns\n",
    "DataProcessor.box_plot_for_object_columns = box_plot_for_object_columns\n",
    "DataProcessor.scatter_plot_for_numeric_columns = scatter_plot_for_numeric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.6: Split dataset to train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(self, valid_size=0.2, random_state=None):\n",
    "    \"\"\"Split the data into training and validation sets, ensuring all unique 'Model' values are present in the training set.\"\"\"\n",
    "    train_set = copy.deepcopy(self)\n",
    "    valid_set = copy.deepcopy(self)\n",
    "    \n",
    "    # Set random seed if provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Identify unique models\n",
    "    unique_models = self.data['Model'].unique()\n",
    "    \n",
    "    # Create training indices: start with an empty list\n",
    "    train_indices = []\n",
    "    \n",
    "    # Iterate through each unique model\n",
    "    for model in unique_models:\n",
    "        # Get indices where the 'Model' column equals the current unique model\n",
    "        model_indices = self.data[self.data['Model'] == model].index.tolist()\n",
    "        \n",
    "        # Randomly choose one index for each model to be included in the training set\n",
    "        # This ensures that at least one sample of each model is in the training set\n",
    "        chosen_index = np.random.choice(model_indices)\n",
    "        train_indices.append(chosen_index)\n",
    "\n",
    "    # Create remaining indices for the training set\n",
    "    remaining_count = int((1 - valid_size) * len(self.data)) - len(train_indices)\n",
    "    remaining_indices = list(set(np.arange(len(self.data))) - set(train_indices))\n",
    "    \n",
    "    # Randomly sample from the remaining indices to complete the training set\n",
    "    sampled_indices = np.random.choice(remaining_indices, size=remaining_count, replace=False).tolist()\n",
    "    train_indices.extend(sampled_indices)\n",
    "    \n",
    "    # Create validation indices: the indices not in the training set\n",
    "    valid_indices = list(set(np.arange(len(self.data))) - set(train_indices))\n",
    "    \n",
    "    # Shuffle the training indices\n",
    "    np.random.shuffle(train_indices)\n",
    "\n",
    "    # Assign data to train_set and valid_set\n",
    "    train_set.data = self.data.iloc[train_indices].reset_index(drop=True)\n",
    "    valid_set.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "    return train_set, valid_set\n",
    "\n",
    "DataProcessor.train_valid_split = train_valid_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.7: Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(self):\n",
    "    \"\"\"\n",
    "    Mã hóa các biến theo yêu cầu:\n",
    "    - Hãng xe (Make): thay thế bằng giá trung vị của các xe thuộc hãng đó\n",
    "    - Mẫu xe (Model): thay thế bằng giá trung vị của các xe cùng mẫu\n",
    "    - Loại nhiên liệu (Fuel Type): Hybrid -> 1; Ngược lại -> 0\n",
    "    - Hộp số (Transmission): Auto -> 1; Manual -> 0\n",
    "    - Địa điểm (Location): nếu giá trung vị của Location ≥ giá trung vị toàn cục -> 1; ngược lại -> 0\n",
    "    - Màu sắc (Color): Black, Blue, Yellow -> 1; ngược lại -> 0\n",
    "    - Loại người bán (Seller Type): Corporate -> 1; Individual và Commercial Registration -> 0\n",
    "    - Drivetrain: FWD -> 1, RWD -> 2, AWD -> 3\n",
    "    \"\"\"\n",
    "    # Hãng xe: map giá trung vị theo Make\n",
    "    make_median = self.data.groupby('Make')['Price'].median()\n",
    "    self.data['Make'] = self.data['Make'].map(make_median)\n",
    "    \n",
    "    # Mẫu xe: map giá trung vị theo Model\n",
    "    model_median = self.data.groupby('Model')['Price'].median()\n",
    "    self.data['Model'] = self.data['Model'].map(model_median)\n",
    "    \n",
    "    # Loại nhiên liệu: chỉ giữ Hybrid = 1, còn lại = 0\n",
    "    self.data['Fuel Type'] = self.data['Fuel Type'].apply(lambda x: 1 if x.strip().lower() == 'hybrid' else 0)\n",
    "    \n",
    "    # Hộp số: Auto = 1; Manual = 0\n",
    "    self.data['Transmission'] = self.data['Transmission'].apply(lambda x: 1 if 'auto' in x.strip().lower() else 0)\n",
    "    \n",
    "    # Địa điểm: chia theo giá trung vị so với toàn bộ data\n",
    "    global_median_price = self.data['Price'].median()\n",
    "    location_medians = self.data.groupby('Location')['Price'].median()\n",
    "    def encode_location(loc):\n",
    "        return 1 if location_medians.loc[loc] >= global_median_price else 0\n",
    "    self.data['Location'] = self.data['Location'].apply(encode_location)\n",
    "    \n",
    "    # Màu sắc: Black, Blue, Yellow -> 1; khác -> 0\n",
    "    self.data['Color'] = self.data['Color'].apply(lambda x: 1 if x.strip().lower() in ['black', 'blue', 'yellow'] else 0)\n",
    "    \n",
    "    # Loại người bán: Corporate = 1; Individual và Commercial Registration = 0\n",
    "    self.data['Seller Type'] = self.data['Seller Type'].apply(lambda x: 1 if x.strip().lower() == 'corporate' else 0)\n",
    "    \n",
    "    # Drivetrain: FWD = 1, RWD = 2, AWD = 3\n",
    "    drivetrain_mapping = {'FWD': 1, 'RWD': 2, 'AWD': 3}\n",
    "    self.data['Drivetrain'] = self.data['Drivetrain'].map(drivetrain_mapping)\n",
    "    \n",
    "    print(\"Data encoding completed!\")\n",
    "    encoding_information = {}\n",
    "    encoding_information['make_median'] = make_median\n",
    "    encoding_information['model_median'] = model_median\n",
    "    encoding_information['location_medians'] = location_medians\n",
    "    encoding_information['global_median_price'] = global_median_price\n",
    "    encoding_information['drivetrain_mapping'] = drivetrain_mapping\n",
    "    return encoding_information\n",
    "    \n",
    "# Gắn hàm encode_data vào class DataProcessor\n",
    "DataProcessor.encode_data = encode_data\n",
    "\n",
    "def encode_data_test_set(self, encoding_information):\n",
    "    \"\"\"\n",
    "    Mã hóa dữ liệu kiểm tra theo thông tin đã học từ tập huấn luyện:\n",
    "    - Hãng xe (Make): thay thế bằng giá trung vị của các xe thuộc hãng đó\n",
    "    - Mẫu xe (Model): thay thế bằng giá trung vị của các xe cùng mẫu\n",
    "    - Loại nhiên liệu (Fuel Type): Hybrid -> 1; Ngược lại -> 0\n",
    "    - Hộp số (Transmission): Auto -> 1; Manual -> 0\n",
    "    - Địa điểm (Location): nếu giá trung vị của Location ≥ giá trung vị toàn cục -> 1; ngược lại -> 0\n",
    "    - Màu sắc (Color): Black, Blue, Yellow -> 1; ngược lại -> 0\n",
    "    - Loại người bán (Seller Type): Corporate -> 1; Individual và Commercial Registration -> 0\n",
    "    - Drivetrain: FWD -> 1, RWD -> 2, AWD -> 3\n",
    "    \"\"\"\n",
    "    # Hãng xe: map giá trung vị theo Make, use global_median_price for missing keys\n",
    "    self.data['Make'] = self.data['Make'].apply(\n",
    "        lambda x: encoding_information['make_median'].get(x, encoding_information['global_median_price'])\n",
    "    )\n",
    "    \n",
    "    # Mẫu xe: map giá trung vị theo Model, use global_median_price for missing keys\n",
    "    self.data['Model'] = self.data['Model'].apply(\n",
    "        lambda x: encoding_information['model_median'].get(x, encoding_information['global_median_price'])\n",
    "    )\n",
    "    # Loại nhiên liệu: chỉ giữ Hybrid = 1, còn lại = 0\n",
    "    self.data['Fuel Type'] = self.data['Fuel Type'].apply(lambda x: 1 if x.strip().lower() == 'hybrid' else 0)\n",
    "    \n",
    "    # Hộp số: Auto = 1; Manual = 0\n",
    "    self.data['Transmission'] = self.data['Transmission'].apply(lambda x: 1 if 'auto' in x.strip().lower() else 0)\n",
    "    \n",
    "    # Địa điểm: chia theo giá trung vị so với toàn bộ data\n",
    "    global_median_price = encoding_information['global_median_price']\n",
    "    location_medians = encoding_information['location_medians']\n",
    "    def encode_location(loc):\n",
    "        try:\n",
    "            return 1 if location_medians.loc[loc] >= global_median_price else 0\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    self.data['Location'] = self.data['Location'].apply(encode_location)\n",
    "\n",
    "    # Màu sắc: Black, Blue, Yellow -> 1; khác -> 0\n",
    "    self.data['Color'] = self.data['Color'].apply(lambda x: 1 if x.strip().lower() in ['black', 'blue', 'yellow'] else 0)\n",
    "\n",
    "    # Loại người bán: Corporate = 1; Individual và Commercial Registration = 0\n",
    "    self.data['Seller Type'] = self.data['Seller Type'].apply(lambda x: 1 if x.strip().lower() == 'corporate' else 0)\n",
    "\n",
    "    # Drivetrain: FWD = 1, RWD = 2, AWD = 3\n",
    "    drivetrain_mapping = encoding_information['drivetrain_mapping']\n",
    "    self.data['Drivetrain'] = self.data['Drivetrain'].map(drivetrain_mapping)\n",
    "\n",
    "    print(\"Data encoding completed!\")\n",
    "\n",
    "# Gắn hàm encode_data_test_set vào class DataProcessor\n",
    "DataProcessor.encode_data_test_set = encode_data_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.8: Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(self):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa các feature dữ liệu:\n",
    "    - Áp dụng log transformation cho 'Price' và 'Kilometer' nhằm giảm ảnh hưởng do độ lệch quy mô.\n",
    "    - Sau đó dùng MinMaxScaler chuẩn hóa toàn bộ các biến số.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Áp dụng log1p (log(1+x)) để tránh lỗi với giá trị 0\n",
    "    self.data['Kilometer'] = np.log1p(self.data['Kilometer'])\n",
    "    \n",
    "    # Chọn các cột số để scale\n",
    "    numeric_cols = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    numeric_cols = numeric_cols.drop(['Price'])\n",
    "    \n",
    "    min_vals = self.data[numeric_cols].min()\n",
    "    max_vals = self.data[numeric_cols].max()\n",
    "    range_vals = max_vals - min_vals\n",
    "    # Avoid division by zero by replacing 0 differences with 1\n",
    "    range_vals[range_vals == 0] = 1\n",
    "    self.data[numeric_cols] = (self.data[numeric_cols] - min_vals) / range_vals\n",
    "    \n",
    "    self.unique_values()\n",
    "    print(\"Data normalization completed!\")\n",
    "\n",
    "    return range_vals, min_vals, max_vals\n",
    "    \n",
    "DataProcessor.normalize_data = normalize_data\n",
    "\n",
    "def normalize_data_test_set(self, range_vals, min_vals):\n",
    "    \"\"\"\n",
    "    Chuẩn hóa dữ liệu kiểm tra theo thông tin đã học từ tập huấn luyện:\n",
    "    - Áp dụng log transformation cho 'Price' và 'Kilometer' nhằm giảm ảnh hưởng do độ lệch quy mô.\n",
    "    - Sau đó dùng MinMaxScaler chuẩn hóa toàn bộ các biến số.\"\n",
    "    \"\"\"\n",
    "    # Áp dụng log1p (log(1+x)) để tránh lỗi với giá trị 0\n",
    "    self.data['Kilometer'] = np.log1p(self.data['Kilometer'])\n",
    "    \n",
    "    # Chọn các cột số để scale\n",
    "    numeric_cols = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    numeric_cols = numeric_cols.drop(['Price'])\n",
    "    \n",
    "    # Normalize the test set\n",
    "    self.data[numeric_cols] = (self.data[numeric_cols] - min_vals) / range_vals\n",
    "\n",
    "    self.unique_values()\n",
    "    print(\"Data normalization completed!\")\n",
    "\n",
    "DataProcessor.normalize_data_test_set = normalize_data_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Load and Explore data (train.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will load and explore some information about the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T14:30:37.167466Z",
     "start_time": "2025-03-18T14:30:36.972327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the DataProcessor class and load the data (train.csv)\n",
    "file_path = './data/train.csv'\n",
    "data = DataProcessor(file_path)\n",
    "data.load_data()\n",
    "\n",
    "# Print summary of the data\n",
    "print(\"\\nSummary of the data:\")\n",
    "data.get_summary()\n",
    "\n",
    "# First 5 rows of data\n",
    "print(\"\\nFirst 5 rows of data:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T14:30:52.574475Z",
     "start_time": "2025-03-18T14:30:52.524118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print information about the null values\n",
    "data.null_info()\n",
    "\n",
    "# Print information about the columns\n",
    "data.get_column_initial_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values of some columns\n",
    "data.get_some_unique_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we split data into a training set and a validation set, fill NaN values, and perform some data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.0: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = data.train_valid_split(valid_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and fill missing values\n",
    "train_data.clean_data()\n",
    "valid_data.clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "train_data.transform_data()\n",
    "valid_data.transform_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3: Explore train data after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.get_column_after_transform_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.get_column_after_transform_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find models in validation set that are not in train set\n",
    "train_models = train_data.data['Model'].unique()\n",
    "validation_models = valid_data.data['Model'].unique()\n",
    "\n",
    "new_models = [model for model in validation_models if model not in train_models]\n",
    "\n",
    "print(\"Models in validation set but not in train set:\", new_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.get_some_unique_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique values for each column\n",
    "numeric_columns, object_columns = train_data.unique_values()\n",
    "print()\n",
    "numeric_columns, object_columns = valid_data.unique_values()\n",
    "\n",
    "# Create save point: after preprocessing\n",
    "train_data_after_preprocessing = copy.deepcopy(train_data)\n",
    "validation_data_after_preprocessing = copy.deepcopy(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plot the correlation matrix\n",
    "train_data.plot_corr_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plot the distribution of numeric columns\n",
    "train_data.plot_distribution_of_numeric_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Box plot: Relationship between Price and Object Columns\n",
    "train_data.box_plot_for_object_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scatter Plot: Relationship between Price and Numeric Columns\n",
    "train_data.scatter_plot_for_numeric_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Data Encoding and Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.1: Train set and Validation set\n",
    "We will use train data and validation data (after preprocessing)\n",
    "- train_data_after_preprocessing\n",
    "- validation_data_after_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_after_preprocessing) # after preprocessing\n",
    "numeric_columns, object_columns = train_data.unique_values()\n",
    "print()\n",
    "validation_data = copy.deepcopy(validation_data_after_preprocessing) # after preprocessing\n",
    "numeric_columns, object_columns = validation_data.unique_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2: Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training data \n",
    "encoding_information = train_data.encode_data()\n",
    "train_data.get_column_after_transform_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the validation data\n",
    "validation_data.encode_data_test_set(encoding_information)\n",
    "validation_data.get_column_after_transform_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save point: after encoding\n",
    "train_data_after_encoding = copy.deepcopy(train_data)\n",
    "validation_data_after_encoding = copy.deepcopy(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.3: Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_after_encoding)\n",
    "train_data.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_vals, min_vals, max_vals = train_data.normalize_data()\n",
    "train_data_after_normalization = copy.deepcopy(train_data) # after normalization\n",
    "train_data_after_normalization.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_vals, min_vals, max_vals \n",
    "# print range_vals, min_vals, max_vals  size\n",
    "print(range_vals.size, min_vals.size, max_vals.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation set normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = copy.deepcopy(validation_data_after_encoding)\n",
    "validation_data.data.describe()\n",
    "validation_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.normalize_data_test_set(range_vals, min_vals)\n",
    "validation_data_after_normalization = copy.deepcopy(validation_data) # after normalization\n",
    "validation_data_after_normalization.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_after_normalization.plot_corr_matrix(15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_after_normalization.plot_corr_matrix(15, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Split dataset and tools to evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1: Split dataset\n",
    "Split training set into X_train and y_train. Split validation set into X_val and y_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_after_normalization)\n",
    "validation_data = copy.deepcopy(validation_data_after_normalization)\n",
    "numeric_columns, object_columns = train_data.numeric_columns, train_data.object_columns\n",
    "print(numeric_columns, len(numeric_columns))\n",
    "\n",
    "train_data = train_data.data\n",
    "validation_data = validation_data.data\n",
    "\n",
    "# X_train, y_train from train_data\n",
    "# X_val, y_val from validation_data\n",
    "\n",
    "X_train = train_data.drop(columns=['Price'])\n",
    "y_train = train_data['Price']\n",
    "\n",
    "X_val = validation_data.drop(columns=['Price'])\n",
    "y_val = validation_data['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2: Tools to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SSE = np.sum((y_true - y_pred) ** 2)\n",
    "    SST = np.sum((y_true - np.mean(y_true)) ** 2) \n",
    "    return 1 - (SSE / SST) # R^2 = 1 - SSE/SST = SSR/SST\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    model_eval = pd.DataFrame({\n",
    "        'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score'],\n",
    "        'Value': [mse(y_true, y_pred), rmse(y_true, y_pred), mae(y_true, y_pred), r2_score(y_true, y_pred)]\n",
    "    })\n",
    "    return model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI: Simple Linear Regression model from Statistics's point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Correlation coefficient map, **Model** maybe the best feature for model. We will prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = beta0 + beta1*X\n",
    "# beta1 = Sxx / Sxy\n",
    "# beta0 = mean(Y) - beta1 * mean(X)\n",
    "# Sxx = sum((X - mean(X))^2)\n",
    "# Sxy = sum((X - mean(X)) * (Y - mean(Y)))\n",
    "\n",
    "def simple_linear_regression(X, y):\n",
    "    mean_X = np.mean(X)\n",
    "    mean_y = np.mean(y)\n",
    "    Sxx = np.sum((X - mean_X) ** 2)\n",
    "    Sxy = np.sum((X - mean_X) * (y - mean_y))\n",
    "    beta1 = Sxy / Sxx\n",
    "    beta0 = mean_y - beta1 * mean_X\n",
    "    return beta0, beta1\n",
    "\n",
    "result = []\n",
    "\n",
    "for F in numeric_columns.drop(['Price']):\n",
    "    # Predict the price of a car based on feature F\n",
    "    beta0, beta1 = simple_linear_regression(X_train[F], y_train)\n",
    "    # print(f\"{F} feature: Price = {beta0:.2f} + {beta1:.2f} * {F}\")\n",
    "\n",
    "    # Predict the price of a car based on feature F\n",
    "    y_pred = beta0 + beta1 * X_train[F]\n",
    "\n",
    "    # Evaluate the model on train set\n",
    "    model_eval = evaluate_model(y_train, y_pred)\n",
    "    result.append([F, model_eval, beta0, beta1])\n",
    "\n",
    "Metric = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
    "\n",
    "for i, metric in enumerate(Metric):\n",
    "    best = result[0]\n",
    "    for res in result:\n",
    "        if (res[1].iloc[i, 1] < best[1].iloc[i, 1] and metric != 'R^2 Score') or \\\n",
    "           (res[1].iloc[i, 1] > best[1].iloc[i, 1] and metric == 'R^2 Score'):\n",
    "            best = res\n",
    "    print(f\"Best feature for {metric}: {best[0]}. Value: {best[1].iloc[i, 1]}\")\n",
    "\n",
    "print(f\"\\nConclusion: {best[0]} is the best feature for all metrics\")\n",
    "# => Max Power (best[0]) is the best feature for all metrics \n",
    "id_BestFeature_in_result = [i for i in range(len(result)) if result[i][0] == best[0]][0]\n",
    "\n",
    "beta0 = result[id_BestFeature_in_result][2]\n",
    "beta1 = result[id_BestFeature_in_result][3]\n",
    "F = result[id_BestFeature_in_result][0]\n",
    "print(f\"\\n{F} formula: Price = {beta0:.2f} + {beta1:.2f} * {F}\")\n",
    "\n",
    "print(\"\\nEvaluation metrics on Training Set:\")\n",
    "result[id_BestFeature_in_result][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model on training set:\n",
    "print(f'Best feature: {result[id_BestFeature_in_result][0]}')\n",
    "beta0 = result[id_BestFeature_in_result][2]\n",
    "beta1 = result[id_BestFeature_in_result][3]\n",
    "\n",
    "# Predict the price of a car based on the best feature (train set)\n",
    "y_pred = (beta0 + beta1 * X_train[result[id_BestFeature_in_result][0]]).round().astype(int)\n",
    "df = pd.DataFrame({'Actual': y_train, 'Predicted': y_pred})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation metrics on Validation Set:\")\n",
    "y_pred = beta0 + beta1 * X_val['Model']\n",
    "\n",
    "model_eval = evaluate_model(y_val, y_pred)\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the price of a car based on the best feature (train set)\n",
    "y_pred = (beta0 + beta1 * X_val[result[id_BestFeature_in_result][0]]).round().astype(int)\n",
    "df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot line of best fit\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(X_train['Model'], y_train, color='blue', alpha=0.5, label='Actual Prices')\n",
    "sorted_idx_train = X_train['Model'].argsort()\n",
    "X_sorted_train = X_train['Model'].iloc[sorted_idx_train]\n",
    "y_line_train = beta0 + beta1 * X_sorted_train\n",
    "axes[0].plot(X_sorted_train, y_line_train, color='red', linewidth=2, label='Predicted Line')\n",
    "axes[0].set_xlabel(\"Model (Encoded)\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "axes[0].set_title(\"Training Set: Actual vs Predicted\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(X_val['Model'], y_val, color='green', alpha=0.5, label='Actual Prices')\n",
    "sorted_idx_val = X_val['Model'].argsort()\n",
    "X_sorted_val = X_val['Model'].iloc[sorted_idx_val]\n",
    "y_line_val = beta0 + beta1 * X_sorted_val\n",
    "axes[1].plot(X_sorted_val, y_line_val, color='orange', linewidth=2, label='Predicted Line')\n",
    "axes[1].set_xlabel(\"Model (Encoded)\")\n",
    "axes[1].set_ylabel(\"Price\")\n",
    "axes[1].set_title(\"Validation Set: Actual vs Predicted\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VII: Multiple Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.1 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegression:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.thetas = np.zeros(self.X_train.shape[1])\n",
    "        self.train_predictions = None\n",
    "        self.val_predictions = None\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for given input data\"\"\"\n",
    "        return np.dot(X, self.thetas)\n",
    "\n",
    "    def compute_loss(self, predictions, y):\n",
    "        \"\"\"Calculate mean squared error\"\"\"\n",
    "        return np.mean((predictions - y) ** 2) / 2\n",
    "\n",
    "    def gradient(self, X, predictions, y):\n",
    "        \"\"\"Compute gradients for weight updates\"\"\"\n",
    "        return np.dot(X.T, (predictions - y)) / len(y)\n",
    "\n",
    "    def update_weights(self, learning_rate, gradient):\n",
    "        \"\"\"Update model parameters\"\"\"\n",
    "        return self.thetas - learning_rate * gradient\n",
    "\n",
    "    def train(self, epochs, learning_rate, log_interval, adam=False):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            # Training predictions and loss\n",
    "            self.train_predictions = self.predict(self.X_train)\n",
    "            train_loss = self.compute_loss(self.train_predictions, self.y_train)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            # Validation predictions and loss\n",
    "            self.val_predictions = self.predict(self.X_val)\n",
    "            val_loss = self.compute_loss(self.val_predictions, self.y_val)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            # Update weights\n",
    "            grad = self.gradient(self.X_train, self.train_predictions, self.y_train)\n",
    "            self.thetas = self.update_weights(learning_rate, grad)\n",
    "\n",
    "            if epoch % log_interval == 0:\n",
    "                print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        return self.thetas, self.train_losses, self.val_losses\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return current model parameters\"\"\"\n",
    "        return self.thetas\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(self.train_losses)), self.train_losses,\n",
    "                label='Training Loss', color='blue', linewidth=2)\n",
    "        plt.plot(range(len(self.val_losses)), self.val_losses,\n",
    "                label='Validation Loss', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.title('Training and Validation Loss over Epochs', fontsize=14)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Set log scale for better visualization if losses vary greatly\n",
    "        plt.yscale('log')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.2 Cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation function\n",
    "def k_fold_cross_validation(self, model_class, k, epochs, learning_rate, log_interval, kind=None, adam=False):\n",
    "    losses_of_each_cross = {}\n",
    "    train_set = copy.deepcopy(self)\n",
    "    val_set = copy.deepcopy(self)\n",
    "\n",
    "    for i in range(k):\n",
    "        train_end_1 = int(i/k * len(self.data))\n",
    "        train_start_2 = int((i + 1)/k * len(self.data))\n",
    "\n",
    "        val_start = int(i/k * len(self.data))\n",
    "        val_end = int((i + 1)/k * len(self.data))\n",
    "\n",
    "        train_set.data = pd.concat([self.data.iloc[:train_end_1], self.data.iloc[train_start_2:]], axis=0).copy()\n",
    "        val_set.data = self.data.iloc[val_start:val_end].copy()\n",
    "\n",
    "        train_set.encode_data()\n",
    "        val_set.encode_data()\n",
    "\n",
    "        train_set.normalize_data()\n",
    "        val_set.normalize_data()\n",
    "\n",
    "        train_data = train_set.data\n",
    "        val_data = val_set.data\n",
    "\n",
    "        X_train = train_data.drop(columns=['Price'])\n",
    "        y_train = train_data['Price']\n",
    "        X_train, y_train = process_data(X_train, y_train, kind)\n",
    "\n",
    "        X_val = val_data.drop(columns=['Price'])\n",
    "        y_val = val_data['Price']\n",
    "        X_val, y_val = process_data(X_val, y_val, kind)\n",
    "\n",
    "        print(f\"Cross {i}: \")\n",
    "        multiLR = model_class(X_train, y_train, X_val, y_val)\n",
    "        _, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval, adam)\n",
    "\n",
    "        loss = {\n",
    "            f'Cross {i + 1}': {\n",
    "                'Train loss': train_losses,\n",
    "                'Val loss': val_losses\n",
    "            }\n",
    "        }\n",
    "        losses_of_each_cross.update(loss)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    return losses_of_each_cross\n",
    "\n",
    "DataProcessor.k_fold_cross_validation = k_fold_cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_validation(cross_validation_losses):\n",
    "    k = len(cross_validation_losses)\n",
    "\n",
    "    ncols = int(np.ceil(np.sqrt(k)))\n",
    "    nrows = int(np.ceil(k / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 4 * nrows), sharex=True, sharey=False)\n",
    "\n",
    "    if nrows == 1:\n",
    "        axes = np.array([axes]) if ncols > 1 else np.array([axes])\n",
    "    else:\n",
    "        axes = np.array(axes)\n",
    "\n",
    "    for idx, (cross, losses) in enumerate(cross_validation_losses.items()):\n",
    "        train_losses = losses['Train loss']\n",
    "        val_losses = losses['Val loss']\n",
    "\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "        row = idx // ncols\n",
    "        col = idx % ncols\n",
    "\n",
    "        axes[row, col].plot(epochs, train_losses, label='Train Loss', linestyle='-', color='blue', alpha=0.7)\n",
    "\n",
    "        axes[row, col].plot(epochs, val_losses, label='Val Loss', linestyle='--', color='orange', alpha=0.7)\n",
    "\n",
    "        axes[row, col].set_title(f'{cross}')\n",
    "        axes[row, col].set_ylabel('Loss')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True)\n",
    "\n",
    "        if row == nrows - 1:\n",
    "            axes[row, col].set_xlabel('Epoch')\n",
    "\n",
    "    for idx in range(k, nrows * ncols):\n",
    "        row = idx // ncols\n",
    "        col = idx % ncols\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.3 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data to fit different methods\n",
    "def process_data(X, y, kind: str):\n",
    "    if kind == 'bias':\n",
    "        X = np.hstack((np.ones(shape=(X.shape[0], 1)), X))\n",
    "    elif kind == 'custom':\n",
    "        y = y / 1e7\n",
    "    elif kind == 'log':\n",
    "        y = np.log(y)\n",
    "    elif kind == 'min_max':\n",
    "        min_y = np.min(y)\n",
    "        max_y = np.max(y)\n",
    "        y = (y - min_y) / (max_y - min_y)\n",
    "    elif kind == 'standardization':\n",
    "        mean_y = np.mean(y)\n",
    "        std_y = np.std(y)\n",
    "        y = (y - mean_y) / std_y\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias\n",
    "X_train_bias = np.hstack((np.ones(shape=(X_train.shape[0], 1)), X_train))\n",
    "X_val_bias = np.hstack((np.ones(shape=(X_val.shape[0], 1)), X_val))\n",
    "\n",
    "# Custom\n",
    "y_train_custom = y_train / 1e7\n",
    "y_val_custom = y_val / 1e7\n",
    "\n",
    "# Log transformation\n",
    "y_train_log = np.log(y_train)\n",
    "y_val_log = np.log(y_val)\n",
    "\n",
    "# Min-Max scaler\n",
    "train_min = np.min(y_train)\n",
    "train_max = np.max(y_train)\n",
    "\n",
    "val_min = np.min(y_val)\n",
    "val_max = np.max(y_val)\n",
    "\n",
    "y_train_scaled = (y_train - train_min) / (train_max - train_min)\n",
    "y_val_scaled = (y_val - val_min) / (val_max - val_min)\n",
    "\n",
    "# Standardization\n",
    "train_mean = np.mean(y_train)\n",
    "train_std = np.std(y_train)\n",
    "val_mean = np.mean(y_val)\n",
    "val_std = np.std(y_val)\n",
    "\n",
    "y_train_standardized = (y_train - train_mean) / train_std\n",
    "y_val_standardized = (y_val - val_mean) / val_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to record results\n",
    "evaluations = []\n",
    "evaluations_label_normalized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experimenting different methods to find out which one works out the best. Base model is using all features and no bias. Next models are built to be expected to outperform this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without bias (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "learning_rate = 0.2\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train, y_train, X_val, y_val)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['base'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = None\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sudden spike in some crosses is most likely due to noise or mismatch in distributions between train set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val)\n",
    "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a bias parameter to increase model's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "learning_rate = 0.2\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train_bias, y_train, X_val_bias, y_val)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['bias'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = 'bias'\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Normalization\n",
    "So far, we have been using normalized features to predict the original labels. In this section, we will explore scaling the labels to a smaller range to facilitate more efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.2\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train_bias, y_train_custom, X_val_bias, y_val_custom)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['custom'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = 'custom'\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the training speed up incredibly fast, with the model converging in just approximately 50 epochs. Although it seems the loss can decrease a bit more, model has nearly reached the minimum so the loss will be updated slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "evaluations_label_normalized.append(evaluate_model(y_val_custom, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, predictions have to be denormalized and evaluated again on original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retransform to initial range\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "y_pred_original = y_pred * 1e7\n",
    "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train_bias, y_train_log, X_val_bias, y_val_log)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['log'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = 'log'\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "evaluations_label_normalized.append(evaluate_model(y_val_log, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retransform to initial range\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "y_pred_original = np.exp(y_pred)\n",
    "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train_bias, y_train_scaled, X_val_bias, y_val_scaled)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['min_max'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = 'min_max'\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "evaluations_label_normalized.append(evaluate_model(y_val_scaled, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retransform to initial range\n",
    "y_pred_original = y_pred * (train_max - train_min) + train_min\n",
    "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "learning_rate = 0.2\n",
    "log_interval = epochs / 10\n",
    "thetas, train_losses, val_losses = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLR = MultiLinearRegression(X_train_bias, y_train_standardized, X_val_bias, y_val_standardized)\n",
    "thetas, train_losses, val_losses = multiLR.train(epochs, learning_rate, log_interval)\n",
    "models['standardization'] = multiLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "k = 4\n",
    "kind = 'standardization'\n",
    "cross_validation_losses = {}\n",
    "cross_validation_losses = train_data_after_preprocessing.k_fold_cross_validation(MultiLinearRegression, k, epochs, learning_rate, log_interval, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation(cross_validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple metrics\n",
    "y_pred = multiLR.predict(X_val_bias)\n",
    "evaluations_label_normalized.append(evaluate_model(y_val_standardized, y_pred).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retransform to initial range\n",
    "y_pred_original = y_pred * val_std + val_mean\n",
    "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII.5 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation summary\n",
    "summary = pd.concat(evaluations, axis=1)\n",
    "summary.index = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
    "summary.columns = ['Base', 'Bias', 'Custom', 'Log transformation', 'Min-Max scaler', 'Standardization']\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation summary on normalized labels\n",
    "summary_label_normalized = pd.concat(evaluations_label_normalized, axis=1)\n",
    "summary_label_normalized.index = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
    "summary_label_normalized.columns = ['Custom', 'Log transformation', 'Min-Max scaler', 'Standardization']\n",
    "summary_label_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with various approaches, the model with bias achieved the highest R² score. The elevated loss values can be attributed to the significant disparity in the range between the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_after_preprocessing.data['Price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the high R² score indicates that the model can account for 96.7% of the variance in the data, demonstrating that the base model, the model with bias, and the standardization method are all performing effectively. Ultimately, the model with bias was chosen as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['bias']\n",
    "y_pred = model.predict(X_val_bias)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VIII: Polynomial Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialFeatures:\n",
    "    \"\"\"\n",
    "    Tạo ma trận đặc trưng đa thức.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    degree : int, mặc định=2\n",
    "        Bậc của đa thức. \n",
    "    \n",
    "    include_bias : bool, mặc định=True\n",
    "        Nếu True, thêm cột toàn 1 vào ma trận (hằng số).\n",
    "    \n",
    "    interaction_only : bool, mặc định=False\n",
    "        Nếu True, chỉ bao gồm các tương tác giữa các đặc trưng.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree=2, include_bias=True, interaction_only=False):\n",
    "        self.degree = degree\n",
    "        self.include_bias = include_bias\n",
    "        self.interaction_only = interaction_only\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Tính số lượng đặc trưng đầu ra.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng đầu vào.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        n_samples, n_features = np.asarray(X).shape\n",
    "        self.n_input_features_ = n_features\n",
    "        \n",
    "        # Tính số lượng đặc trưng đầu ra\n",
    "        combinations = []\n",
    "        for d in range(0, self.degree + 1):\n",
    "            if d == 0 and not self.include_bias:\n",
    "                continue\n",
    "            if d == 1:  # không thay đổi các đặc trưng ban đầu\n",
    "                combinations.extend(range(n_features))\n",
    "                continue\n",
    "                \n",
    "            if self.interaction_only:\n",
    "                combinations.extend(combinations_with_replacement(range(n_features), d))\n",
    "            else:\n",
    "                combinations.extend([c for c in combinations_with_replacement(range(n_features), d) \n",
    "                                    if len(set(c)) == 1])\n",
    "        \n",
    "        self.n_output_features_ = len(combinations) + (1 if self.include_bias else 0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Chuyển đổi đặc trưng thành đặc trưng đa thức.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng đầu vào.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        XP : np.ndarray, shape (n_samples, n_output_features)\n",
    "            Ma trận đặc trưng đa thức.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_features != self.n_input_features_:\n",
    "            raise ValueError(\"X shape does not match training shape\")\n",
    "        \n",
    "        # Khởi tạo ma trận đầu ra\n",
    "        XP = np.ones((n_samples, 0))\n",
    "        \n",
    "        # Thêm hằng số nếu include_bias=True\n",
    "        if self.include_bias:\n",
    "            XP = np.hstack((np.ones((n_samples, 1)), XP))\n",
    "        \n",
    "        # Thêm đặc trưng ban đầu (bậc 1)\n",
    "        if self.degree >= 1:\n",
    "            XP = np.hstack((XP, X))\n",
    "        \n",
    "        # Tạo đặc trưng đa thức bậc cao hơn\n",
    "        for d in range(2, self.degree + 1):\n",
    "            if self.interaction_only:\n",
    "                combs = [c for c in combinations_with_replacement(range(n_features), d)\n",
    "                         if len(set(c)) > 1]\n",
    "            else:\n",
    "                combs = list(combinations_with_replacement(range(n_features), d))\n",
    "            \n",
    "            for comb in combs:\n",
    "                new_col = np.ones((n_samples, 1))\n",
    "                for i in comb:\n",
    "                    new_col = new_col * X[:, i:i+1]\n",
    "                XP = np.hstack((XP, new_col))\n",
    "        \n",
    "        return XP\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit và transform cùng một lúc.\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Hồi quy tuyến tính bằng OLS (Ordinary Least Squares).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : bool, mặc định=True\n",
    "        Có tính hằng số hay không.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Huấn luyện mô hình hồi quy tuyến tính.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng.\n",
    "        \n",
    "        y : array-like, shape (n_samples,)\n",
    "            Vector mục tiêu.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Add a column of ones for the intercept if fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X_with_intercept = np.column_stack((np.ones(X.shape[0]), X))\n",
    "        else:\n",
    "            X_with_intercept = X\n",
    "\n",
    "        # Calculate coefficients using the normal equation: beta = (X^T X)^(-1) X^T y\n",
    "        beta, residues, rank, s = np.linalg.lstsq(X_with_intercept, y, rcond=None)\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = beta[0]\n",
    "            self.coef_ = beta[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = beta\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Dự đoán sử dụng mô hình đã huấn luyện.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Giá trị dự đoán.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        return X.dot(self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression:\n",
    "    \"\"\"\n",
    "    Mô hình hồi quy đa thức.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    degree : int, mặc định=2\n",
    "        Bậc của đa thức.\n",
    "    \n",
    "    include_bias : bool, mặc định=True\n",
    "        Có thêm cột toàn 1 vào ma trận không.\n",
    "    \n",
    "    interaction_only : bool, mặc định=False\n",
    "        Chỉ bao gồm các tương tác giữa các đặc trưng.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree=2, include_bias=True, interaction_only=False):\n",
    "        self.degree = degree\n",
    "        self.include_bias = include_bias\n",
    "        self.interaction_only = interaction_only\n",
    "        self.poly = PolynomialFeatures(degree=degree, \n",
    "                                       include_bias=include_bias,\n",
    "                                       interaction_only=interaction_only)\n",
    "        self.linear_regression = LinearRegression(fit_intercept=False if include_bias else True)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Huấn luyện mô hình hồi quy đa thức.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng.\n",
    "        \n",
    "        y : array-like, shape (n_samples,)\n",
    "            Vector mục tiêu.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "        self.linear_regression.fit(X_poly, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Dự đoán sử dụng mô hình đã huấn luyện.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Ma trận đặc trưng.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Giá trị dự đoán.\n",
    "        \"\"\"\n",
    "        X_poly = self.poly.transform(X)\n",
    "        return self.linear_regression.predict(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2: Training and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v3 = train_data.drop(columns=['Price'])\n",
    "y_train_v3 = train_data['Price']\n",
    "\n",
    "X_val_v3 = validation_data.drop(columns=['Price'])\n",
    "y_val_v3 = validation_data['Price']\n",
    "\n",
    "model = PolynomialRegression(degree=1)\n",
    "model.fit(X_train_v3, y_train_v3)\n",
    "    \n",
    "y_pred = model.predict(X_val_v3)\n",
    "model_eval = evaluate_model(y_val_v3, y_pred)\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Scatter plot of predicted vs actual\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_val_v3, y_pred, alpha=0.5)\n",
    "plt.plot([y_val_v3.min(), y_val_v3.max()], [y_val_v3.min(), y_val_v3.max()], 'r--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted')\n",
    "\n",
    "# Plot 2: Same scatter plot with log scale (helps with extreme values)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_val_v3, y_pred, alpha=0.5)\n",
    "plt.plot([y_val_v3.min(), y_val_v3.max()], [y_val_v3.min(), y_val_v3.max()], 'r--')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Actual Price (log scale)')\n",
    "plt.ylabel('Predicted Price (log scale)')\n",
    "plt.title('Actual vs Predicted - Log Scale')\n",
    "\n",
    "# Plot 3: Histogram of errors\n",
    "plt.subplot(1, 3, 3)\n",
    "errors = y_pred - y_val_v3\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IX: Linear Regression model with PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
